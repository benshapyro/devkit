# AI Maturity Assessment Framework

This framework provides structured questions for evaluating organizational AI maturity across four phases. Questions progress from foundational to advanced capabilities within each phase.

## Assessment Instructions

**For each question:**
- Score on 0-5 scale: 0=None, 1=Aware, 2=Piloting, 3=Implemented, 4=Scaling, 5=Optimized
- Request specific evidence and examples
- Note qualitative observations about culture and readiness
- Identify blockers and accelerators

**Interview Approach:**
- Start with open questions to establish context
- Follow up with specific capability probes
- Request artifacts (docs, screenshots, process flows)
- Validate stated capabilities with behavioral evidence

---

## Phase 1: Foundations (Governance, Data Access, Executive Alignment)

### Executive Sponsorship & Strategic Alignment

**Q1.1** Do you have an identified executive sponsor for AI initiatives? Who is it and what is their level of involvement?
- Look for: Named sponsor at C-level or VP+, active participation, budget authority
- Evidence: Meeting cadence, decision logs, strategic alignment documents

**Q1.2** Has AI been formally incorporated into your organization's strategic plan?
- Look for: Written strategic objectives, board-level discussion, resource allocation
- Evidence: Strategic plan documents, board presentations, budget allocations

**Q1.3** Is there a cross-functional steering committee for AI governance?
- Look for: Defined membership (business + IT + legal + compliance), regular meetings, decision authority
- Evidence: Charter document, meeting minutes, escalation examples

**Q1.4** How does your organization measure and track AI initiative success?
- Look for: Defined KPIs, tracking mechanisms, regular reporting to leadership
- Evidence: Dashboards, quarterly reviews, ROI calculations

**Q1.5** What is the annual budget allocated to AI initiatives?
- Look for: Dedicated budget line, flexibility for experimentation, multi-year commitment
- Evidence: Budget documents, spending history, approval processes

### Data Infrastructure & Access

**Q2.1** What percentage of your critical business data is accessible and AI-ready?
- Look for: Data catalog, quality standards, documented access patterns
- Evidence: Data inventory, quality reports, usage analytics

**Q2.2** Do you have data classification policies that define what can be used for AI?
- Look for: Clear classification framework, documented policies, automated enforcement
- Evidence: Policy documents, classification examples, access control logs

**Q2.3** What data governance processes are in place?
- Look for: Data stewardship roles, quality processes, lineage tracking
- Evidence: Governance charter, steward assignments, quality metrics

**Q2.4** How do teams request and gain access to data for AI projects?
- Look for: Clear request process, defined SLAs, automated provisioning
- Evidence: Request forms, approval workflows, turnaround time metrics

**Q2.5** What technical infrastructure exists for AI development and deployment?
- Look for: ML platforms, compute resources, deployment pipelines
- Evidence: Architecture diagrams, capacity plans, utilization metrics

### Governance Frameworks & Decision Rights

**Q3.1** Do you have a documented AI governance framework?
- Look for: Written policies, decision rights, escalation paths, risk thresholds
- Evidence: Governance documentation, approval workflows, decision logs

**Q3.2** How are AI use cases approved or rejected?
- Look for: Clear criteria, consistent process, defined approval authority
- Evidence: Approval templates, decision records, rejection rationale examples

**Q3.3** What is the typical time from use case submission to approval?
- Look for: Defined SLA, measured performance, continuous improvement
- Evidence: Process metrics, bottleneck analysis, improvement initiatives

**Q3.4** Who has authority to make AI-related decisions at different risk levels?
- Look for: Tiered decision framework, clear delegation, risk-based routing
- Evidence: Decision matrix, delegation documentation, risk assessment examples

**Q3.5** Have you established an AI Center of Excellence or similar coordinating body?
- Look for: Defined charter, dedicated resources, cross-functional membership
- Evidence: CoE documentation, resource allocation, activity reports

### Risk Management & Compliance

**Q4.1** What responsible AI principles has your organization adopted?
- Look for: Written principles, specific to your context, operationalized in processes
- Evidence: Principle documentation, training materials, compliance checks

**Q4.2** How do you assess and mitigate AI-related risks?
- Look for: Risk assessment framework, documented process, mitigation tracking
- Evidence: Risk assessment templates, mitigation plans, audit trails

**Q4.3** What processes exist for bias detection and mitigation?
- Look for: Systematic evaluation, diverse perspectives, continuous monitoring
- Evidence: Bias testing protocols, review committee, remediation examples

**Q4.4** How do you ensure AI systems comply with relevant regulations?
- Look for: Regulatory mapping, compliance checks, audit readiness
- Evidence: Compliance documentation, audit reports, training records

**Q4.5** What incident response processes exist for AI failures?
- Look for: Defined playbook, escalation procedures, post-mortem process
- Evidence: Incident response plan, past incident reviews, improvement actions

---

## Phase 2: AI Fluency (Literacy, Champions, Learning Networks)

### Workforce AI Literacy & Skills

**Q5.1** What percentage of your workforce has been trained on AI fundamentals?
- Look for: Measured participation, completion rates, skill validation
- Evidence: Training records, completion dashboards, assessment results

**Q5.2** Are training programs tailored by role and function?
- Look for: Role-specific content, relevant use cases, hands-on practice
- Evidence: Curriculum maps, role-based modules, feedback scores

**Q5.3** How do you measure AI fluency across the organization?
- Look for: Assessment methodology, baseline measurement, progress tracking
- Evidence: Fluency metrics, trend analysis, capability maps

**Q5.4** What percentage of employees actively use AI tools in their daily work?
- Look for: Usage tracking, adoption metrics, behavior change evidence
- Evidence: Tool analytics, usage reports, productivity impact data

**Q5.5** Do technical teams (engineering, data science) have advanced AI/ML skills?
- Look for: Demonstrated capability, certification/training, production deployments
- Evidence: Team skill assessments, project portfolios, peer reviews

### Champion Networks & Knowledge Sharing

**Q6.1** Have you established an AI champion network?
- Look for: Identified champions, defined role, support structure
- Evidence: Champion roster, role description, activity metrics

**Q6.2** How do champions share knowledge and best practices?
- Look for: Regular sessions, documentation, community building
- Evidence: Session schedules, shared resources, community metrics

**Q6.3** What mechanisms exist for cross-functional learning?
- Look for: Communities of practice, cross-team showcases, knowledge base
- Evidence: Community charters, showcase calendars, knowledge repositories

**Q6.4** How are successful AI use cases documented and shared?
- Look for: Case study templates, sharing rituals, searchable repository
- Evidence: Case study library, showcase recordings, reuse metrics

**Q6.5** Do champions receive recognition and career benefits for their role?
- Look for: Formal recognition, performance integration, career pathing
- Evidence: Recognition programs, performance criteria, promotion examples

### Training Programs & Learning Infrastructure

**Q7.1** What formal AI training programs are available?
- Look for: Structured curriculum, multiple levels, continuous learning
- Evidence: Course catalog, completion rates, skill progression

**Q7.2** How frequently is training content updated?
- Look for: Regular refresh cycle, incorporation of new capabilities, feedback loops
- Evidence: Update schedule, version history, feedback integration

**Q7.3** What hands-on learning opportunities exist?
- Look for: Sandboxes, hackathons, real project involvement
- Evidence: Sandbox environments, hackathon schedule, project assignments

**Q7.4** How do you measure training effectiveness?
- Look for: Knowledge assessments, behavior change metrics, business impact
- Evidence: Assessment results, usage correlation, impact studies

**Q7.5** What resources are available for self-directed learning?
- Look for: Curated resources, access to platforms, time allocation
- Evidence: Resource library, platform subscriptions, learning time policies

### Cultural Readiness & Experimentation

**Q8.1** Does your culture encourage experimentation with AI?
- Look for: Psychological safety, failure tolerance, innovation time
- Evidence: Innovation programs, failure retrospectives, time allocation policies

**Q8.2** How do you celebrate and reward AI innovation?
- Look for: Recognition programs, incentive alignment, visibility of wins
- Evidence: Award programs, incentive structures, communication examples

**Q8.3** What barriers prevent employees from experimenting with AI?
- Look for: Awareness of barriers, mitigation efforts, ongoing monitoring
- Evidence: Barrier assessment, mitigation plans, progress tracking

**Q8.4** How do leaders model AI adoption?
- Look for: Visible usage, public support, resource commitment
- Evidence: Leadership communication, tool usage data, budget allocation

**Q8.5** What percentage of employees see AI as an opportunity vs. a threat?
- Look for: Sentiment tracking, communication strategy, change management
- Evidence: Surveys, focus groups, communication plans

---

## Phase 3: Scope & Prioritize (Capture Ideas, Systematic Evaluation)

### Use Case Identification & Intake

**Q9.1** Do you have a formal process for submitting AI use case ideas?
- Look for: Defined intake process, accessible to all, clear requirements
- Evidence: Intake forms, submission portal, process documentation

**Q9.2** How many use cases have been submitted through this process?
- Look for: Volume of submissions, trend over time, source distribution
- Evidence: Submission logs, trend analysis, source breakdown

**Q9.3** What is the typical time from submission to initial evaluation?
- Look for: Defined SLA, measured performance, continuous improvement
- Evidence: Process metrics, turnaround tracking, improvement initiatives

**Q9.4** How do you ensure diverse use cases from across the organization?
- Look for: Active outreach, balanced representation, targeted discovery
- Evidence: Source diversity metrics, discovery sessions, proactive scanning

**Q9.5** What support is provided to help refine use case proposals?
- Look for: Templates, guidance, review sessions, improvement feedback
- Evidence: Submission templates, refinement workshops, feedback examples

### Prioritization Frameworks & Decision Criteria

**Q10.1** What criteria do you use to evaluate and prioritize use cases?
- Look for: Multi-dimensional framework, transparent criteria, consistent application
- Evidence: Scoring rubric, evaluation examples, decision rationale

**Q10.2** How do you balance quick wins vs. strategic initiatives?
- Look for: Explicit portfolio strategy, mix targets, risk diversification
- Evidence: Portfolio analysis, mix metrics, strategic alignment

**Q10.3** Do you consider reusability potential when prioritizing?
- Look for: Reusability assessment, pattern identification, future leverage
- Evidence: Reusability scoring, pattern catalog, leverage examples

**Q10.4** How do you involve both technical and business stakeholders in prioritization?
- Look for: Cross-functional review, balanced perspectives, collaborative decision-making
- Evidence: Review team composition, meeting notes, consensus processes

**Q10.5** What happens to use cases that are not immediately prioritized?
- Look for: Backlog management, periodic review, communication to submitters
- Evidence: Backlog tracking, review cadence, communication examples

### Backlog Management & Resource Allocation

**Q11.1** Do you maintain a visible, prioritized backlog of AI initiatives?
- Look for: Centralized backlog, transparent prioritization, regular updates
- Evidence: Backlog tool, prioritization criteria, update frequency

**Q11.2** How do you allocate resources across the portfolio?
- Look for: Resource planning, capacity management, dynamic reallocation
- Evidence: Resource allocation models, capacity dashboards, reallocation examples

**Q11.3** What is the typical ratio of exploration vs. exploitation work?
- Look for: Defined targets, measurement, balance management
- Evidence: Work categorization, mix metrics, balance adjustments

**Q11.4** How do you manage dependencies between use cases?
- Look for: Dependency mapping, sequencing logic, coordination mechanisms
- Evidence: Dependency diagrams, sequence plans, coordination processes

**Q11.5** What processes exist for retiring or pivoting unsuccessful initiatives?
- Look for: Decision criteria, exit processes, learning capture
- Evidence: Exit thresholds, decision examples, retrospective learnings

### Reusability Pattern Recognition

**Q12.1** Do you identify and catalog reusable components across use cases?
- Look for: Pattern identification, component library, reuse tracking
- Evidence: Component catalog, pattern taxonomy, reuse metrics

**Q12.2** How do you encourage reuse of existing components vs. building new?
- Look for: Reuse-first policy, discovery tools, incentive alignment
- Evidence: Reuse guidelines, search tools, incentive structures

**Q12.3** What technical infrastructure supports component reuse?
- Look for: Component repository, versioning, documentation
- Evidence: Repository tools, version control, documentation standards

**Q12.4** How do you measure and track reuse benefits?
- Look for: Reuse metrics, time/cost savings, quality improvements
- Evidence: Reuse dashboards, savings calculations, quality comparisons

**Q12.5** What governance exists for shared components?
- Look for: Ownership model, maintenance processes, deprecation policies
- Evidence: Ownership documentation, maintenance schedules, deprecation examples

---

## Phase 4: Build & Scale (Iterative Development, Continuous Evaluation)

### Cross-Functional Team Effectiveness

**Q13.1** How are AI development teams structured?
- Look for: Cross-functional composition, clear roles, shared goals
- Evidence: Team charters, role definitions, success metrics

**Q13.2** Do teams include business SMEs throughout development?
- Look for: Active SME involvement, continuous collaboration, validation participation
- Evidence: Team rosters, collaboration patterns, validation sessions

**Q13.3** What is the typical team size and composition?
- Look for: Right-sized teams, appropriate skill mix, executive sponsorship
- Evidence: Team structures, skill inventories, sponsor engagement

**Q13.4** How do you remove blockers and enable team velocity?
- Look for: Escalation processes, blocker tracking, resolution speed
- Evidence: Blocker logs, resolution metrics, escalation examples

**Q13.5** What percentage of AI initiatives involve dedicated cross-functional teams?
- Look for: Team formation patterns, resource dedication, collaboration metrics
- Evidence: Team allocation data, dedication levels, collaboration measures

### Evaluation & Measurement Discipline

**Q14.1** Do you evaluate AI systems before production deployment?
- Look for: Systematic evaluation, defined criteria, gated checkpoints
- Evidence: Evaluation protocols, checkpoint documentation, go/no-go decisions

**Q14.2** What types of evaluation do you conduct?
- Look for: Technical (accuracy, latency), business (value, adoption), operational (cost, reliability)
- Evidence: Evaluation frameworks, test results, decision criteria

**Q14.3** How do you involve SMEs in evaluation?
- Look for: SME review processes, feedback integration, quality validation
- Evidence: Review protocols, feedback examples, quality improvements

**Q14.4** What tools and processes support continuous evaluation?
- Look for: Automated monitoring, dashboards, alert systems
- Evidence: Monitoring tools, dashboard examples, alert configurations

**Q14.5** How do you use evaluation results to improve systems?
- Look for: Feedback loops, iterative improvement, measurable progress
- Evidence: Improvement cycles, before/after metrics, iteration examples

### Production Deployment Capabilities

**Q15.1** How many AI use cases are in production today?
- Look for: Production count, deployment trend, diversity of use cases
- Evidence: Production inventory, deployment timeline, use case catalog

**Q15.2** What is your typical time from proof-of-concept to production?
- Look for: Measured cycle time, bottleneck identification, improvement trends
- Evidence: Cycle time metrics, stage durations, improvement initiatives

**Q15.3** What deployment infrastructure and processes exist?
- Look for: CI/CD pipelines, deployment automation, rollback capabilities
- Evidence: Pipeline documentation, automation scripts, rollback procedures

**Q15.4** How do you monitor production AI systems?
- Look for: Performance monitoring, business metrics, incident detection
- Evidence: Monitoring dashboards, metric definitions, incident logs

**Q15.5** What percentage of projects successfully reach production?
- Look for: Success rate tracking, failure analysis, improvement actions
- Evidence: Project tracking, failure retrospectives, improvement metrics

### Continuous Improvement Practices

**Q16.1** Do you conduct retrospectives on AI initiatives?
- Look for: Regular retrospectives, structured format, action tracking
- Evidence: Retrospective schedule, templates, action items

**Q16.2** How do you capture and share learnings?
- Look for: Knowledge management, accessible documentation, proactive sharing
- Evidence: Knowledge base, documentation standards, sharing mechanisms

**Q16.3** What mechanisms exist for iterating on deployed systems?
- Look for: Update processes, A/B testing, gradual rollouts
- Evidence: Update procedures, testing protocols, rollout strategies

**Q16.4** How do you measure business value from deployed systems?
- Look for: Value tracking, attribution methodology, regular reporting
- Evidence: Value metrics, calculation methods, executive reports

**Q16.5** What is your strategy for keeping pace with evolving AI capabilities?
- Look for: Technology monitoring, upgrade planning, continuous modernization
- Evidence: Technology radar, upgrade roadmaps, modernization initiatives

---

## Scoring Guidelines

### Individual Question Scoring (0-5 Scale)

**0 - None**: No evidence of capability, not on roadmap
**1 - Aware**: Aware of need, exploring options, no concrete plans
**2 - Piloting**: Limited pilot or proof-of-concept, not formalized
**3 - Implemented**: Capability exists and is documented, limited adoption
**4 - Scaling**: Capability deployed broadly, measured and improving
**5 - Optimized**: Industry-leading capability, continuous innovation, measurable excellence

### Phase Scoring Calculation

**Phase Score** = Weighted average of subsection scores
**Subsection Score** = Average of question scores within subsection

**Weight Distribution by Phase:**
- Phase 1: Executive (30%), Data (25%), Governance (25%), Risk (20%)
- Phase 2: Literacy (30%), Champions (25%), Training (25%), Culture (20%)
- Phase 3: Intake (25%), Prioritization (30%), Backlog (25%), Reusability (20%)
- Phase 4: Teams (20%), Evaluation (30%), Production (30%), Improvement (20%)

### Interpretation Notes

**High Variance Within Phase**: Indicates uneven maturity, opportunities for quick improvement
**Low Overall Scores (<40)**: Foundational work needed, 12-18 month transformation ahead
**Phase 1/2 Strong, Phase 3/4 Weak**: Common pattern, systematic scaling challenges
**Isolated High Scores**: May indicate pockets of excellence not yet institutionalized
