# Findings Report

Create comprehensive written documentation that captures all discovery findings, analysis, and recommendations in detail.

## Contents

1. [When to Use](#when-to-use)
2. [Report Structure](#report-structure)
3. [Section Templates](#section-templates)
4. [Content Guidelines](#content-guidelines)
5. [Evidence Standards](#evidence-standards)
6. [Formatting Standards](#formatting-standards)
7. [Generation Process](#generation-process)
8. [Complete Example](#complete-example)

---

## When to Use

User says something like:
- "Write a findings report for [client]"
- "Document our discovery findings"
- "Create a detailed report"
- "I need a comprehensive write-up"

**Typical audience:** Project teams, technical stakeholders, reference document
**Reading context:** Deep dive, reference, archive
**Length:** 10-25 pages

### Report vs. Other Deliverables

| Need | Deliverable |
|------|-------------|
| Quick leadership overview | Executive Summary |
| Presentation to stakeholders | Strategy Deck |
| Implementation planning | Roadmap |
| Detailed documentation | Findings Report |

---

## Report Structure

### Standard Structure (15-20 pages)

```
1. Executive Summary (1 page)
2. Engagement Overview (1-2 pages)
3. Methodology (1 page)
4. Findings by Dimension (6-10 pages)
   - People
   - Process
   - Technology
   - Challenges
   - Solutions
5. Pattern Analysis (2-3 pages)
6. Recommendations (2-3 pages)
7. Roadmap Summary (1 page)
8. Appendices (as needed)
   - Stakeholder List
   - Session Log
   - Data Tables
```

### Section Purposes

| Section | Purpose | Key Content |
|---------|---------|-------------|
| Executive Summary | Standalone overview | Key findings, recommendations, next steps |
| Engagement Overview | Context setting | Scope, objectives, timeline, stakeholders |
| Methodology | Credibility | Approach, framework, data sources |
| Findings | Evidence | What we found, organized by dimension |
| Patterns | Analysis | Themes, correlations, insights |
| Recommendations | Action | Prioritized solutions, rationale |
| Roadmap | Planning | Phased approach, timeline |
| Appendices | Reference | Supporting data, full lists |

---

## Section Templates

### Section 1: Executive Summary

```markdown
# Executive Summary

## Context
[2-3 sentences on why we engaged and scope]

## Key Findings
1. **[Finding headline]** — [One sentence]
2. **[Finding headline]** — [One sentence]
3. **[Finding headline]** — [One sentence]
4. **[Finding headline]** — [One sentence]

## Primary Insight
[2-3 sentences synthesizing findings into core "so what"]

## Recommendations
- **Immediate:** [Quick win with expected outcome]
- **Near-term:** [Foundation item with expected outcome]
- **Strategic:** [Transformation item with expected outcome]

## Next Steps
1. [Specific action] — Owner: [Role], By: [Date]
2. [Specific action] — Owner: [Role], By: [Date]
```

### Section 2: Engagement Overview

```markdown
# Engagement Overview

## Background
[1-2 paragraphs on client context, business situation, why they engaged]

## Objectives
This engagement aimed to:
- [Objective 1]
- [Objective 2]
- [Objective 3]

## Scope
**In Scope:**
- [Area/function covered]
- [Area/function covered]

**Out of Scope:**
- [Explicitly excluded areas]

## Timeline
- **Kickoff:** [Date]
- **Discovery Sessions:** [Date range]
- **Analysis:** [Date range]
- **Deliverables:** [Date]

## Stakeholders Engaged
We conducted [X] discovery sessions with [Y] stakeholders across [departments]:

| Name | Role | Department | Session Date |
|------|------|------------|--------------|
| [Name] | [Title] | [Dept] | [Date] |
| [Name] | [Title] | [Dept] | [Date] |
```

### Section 3: Methodology

```markdown
# Methodology

## Approach
Cadre AI's discovery methodology focuses on five dimensions that determine 
AI readiness and opportunity identification:

1. **People** — Stakeholders, roles, decision-makers, adoption factors
2. **Process** — Workflows, pain points, automation opportunities
3. **Technology** — Systems, integrations, data landscape
4. **Challenges** — Business, operational, technical obstacles
5. **Solutions** — AI opportunities, integration options, recommendations

## Data Collection
- **Discovery Sessions:** Semi-structured interviews with key stakeholders
- **Document Review:** Existing documentation, systems, processes
- **Observation:** [If applicable]

## Analysis Framework
Findings were analyzed using:
- Pattern identification across sessions
- Gap analysis against AI readiness checklist
- Prioritization matrix (Impact × Feasibility)
- Evidence-based confidence scoring

## Confidence Levels
| Level | Definition | Evidence Required |
|-------|------------|-------------------|
| High | Strong evidence, multiple sources | 3+ stakeholders, direct observation |
| Medium | Moderate evidence, inference | 2 stakeholders or strong single source |
| Low | Limited evidence, interpretation | Single mention or indirect evidence |
```

### Section 4: Findings by Dimension

```markdown
# Findings

## People

### Overview
[1-2 paragraphs summarizing people dimension findings]

### Key Stakeholders

| Stakeholder | Role | Archetype | Influence | Notes |
|-------------|------|-----------|-----------|-------|
| [Name] | [Title] | Champion/Skeptic/etc. | High/Medium/Low | [Key note] |

### Findings

#### Finding P1: [Headline as complete thought]
**Confidence:** High/Medium/Low

[2-3 paragraph description of finding]

**Evidence:**
- "[Direct quote]" — [Stakeholder, Date]
- [Observation or data point]

**Implication:** [So what — why this matters]

---

#### Finding P2: [Headline]
[Same structure...]

---

## Process

### Overview
[1-2 paragraphs summarizing process dimension findings]

### Workflow Map
[Description or visual of key workflows examined]

### Findings

#### Finding PR1: [Headline]
**Confidence:** High/Medium/Low

[Description, evidence, implication...]

---

## Technology

### Overview
[1-2 paragraphs summarizing technology dimension findings]

### Systems Inventory

| System | Type | Owner | Integration Status |
|--------|------|-------|-------------------|
| [System] | ERP/CRM/etc. | [Dept] | Integrated/Siloed |

### Findings

#### Finding T1: [Headline]
[Description, evidence, implication...]

---

## Challenges

### Overview
[1-2 paragraphs summarizing challenges identified]

### Challenge Summary

| Challenge | Type | Impact | Urgency | Score |
|-----------|------|--------|---------|-------|
| [Challenge] | Business/Operational/Technical | H/M/L | H/M/L | [X/18] |

### Findings

#### Finding C1: [Headline]
**Priority Score:** [X/18] (Impact: X, Urgency: X, Solvability: X)
**Confidence:** High/Medium/Low

[Description, evidence, implication...]

---

## Solutions

### Overview
[1-2 paragraphs summarizing opportunities identified]

### Opportunity Summary

| Solution | Type | Impact | Feasibility | Score |
|----------|------|--------|-------------|-------|
| [Solution] | AI/Integration/Process | H/M/L | H/M/L | [X/18] |

### Findings

#### Finding S1: [Headline]
**Priority Score:** [X/18] (Impact: X, Feasibility: X, Effort: X)
**Confidence:** High/Medium/Low

[Description, evidence, implication...]
```

### Section 5: Pattern Analysis

```markdown
# Pattern Analysis

## Overview
Analysis across [X] discovery sessions revealed [Y] significant patterns 
that inform our strategic recommendations.

## Dominant Patterns

### Pattern 1: [Pattern Name]
**Strength:** Dominant (7+ evidence points)
**Dimensions:** [Which dimensions this spans]

[2-3 paragraph description of pattern]

**Evidence:**
| Source | Type | Finding |
|--------|------|---------|
| [Session/Person] | Quote/Observation | [Evidence] |
| [Session/Person] | Quote/Observation | [Evidence] |

**Implication:** [What this means for strategy]

---

### Pattern 2: [Pattern Name]
[Same structure...]

---

## Emerging Patterns

### Pattern 3: [Pattern Name]
**Strength:** Emerging (2-4 evidence points)

[Description, evidence, implication — less detail than dominant]

---

## Contradictions & Tensions

| Tension | Perspective A | Perspective B | Resolution |
|---------|---------------|---------------|------------|
| [Topic] | [View 1] | [View 2] | [How to address] |

---

## Coverage Assessment

### Readiness Score: [X]% ([Ready/Nearly Ready/Gaps Remain])

| Dimension | Coverage | Critical Gaps |
|-----------|----------|---------------|
| People | [X]% | [Gap or "None"] |
| Process | [X]% | [Gap or "None"] |
| Technology | [X]% | [Gap or "None"] |
| Challenges | [X]% | [Gap or "None"] |
| Solutions | [X]% | [Gap or "None"] |
```

### Section 6: Recommendations

```markdown
# Recommendations

## Overview
Based on our findings and analysis, we recommend a phased approach focusing 
on quick wins to build momentum, followed by foundational work and strategic 
transformation.

## Recommendation Summary

| # | Recommendation | Priority | Phase | Score |
|---|----------------|----------|-------|-------|
| 1 | [Recommendation] | Quick Win | 1 | [X/18] |
| 2 | [Recommendation] | Strategic | 2 | [X/18] |
| 3 | [Recommendation] | Strategic | 2-3 | [X/18] |

## Detailed Recommendations

### Recommendation 1: [Title]
**Priority:** Quick Win | **Phase:** 1 | **Score:** [X/18]

**What:** [Specific description of recommendation]

**Why:** [Rationale — which challenges this addresses, evidence supporting]

**Expected Impact:**
- [Quantified outcome 1]
- [Quantified outcome 2]

**Dependencies:** [Prerequisites]

**Resources Required:** [Estimate]

**Risks:**
- [Risk 1] — Mitigation: [Approach]

---

### Recommendation 2: [Title]
[Same structure...]
```

### Section 7: Roadmap Summary

```markdown
# Implementation Roadmap

## Overview
[1 paragraph on phased approach]

## Timeline

| Phase | Duration | Focus | Milestone |
|-------|----------|-------|-----------|
| 1: Quick Wins | Weeks 1-4 | [Focus] | [Milestone] |
| 2: Foundation | Months 2-3 | [Focus] | [Milestone] |
| 3: Scale | Months 4-6 | [Focus] | [Milestone] |

## Phase Details

### Phase 1: Quick Wins
**Duration:** Weeks 1-4
**Initiatives:**
- [Initiative 1]
- [Initiative 2]

**Success Metrics:**
- [Metric 1]
- [Metric 2]

[Repeat for Phase 2 and 3...]

## Next Steps
1. [Action] — Owner: [Role], By: [Date]
2. [Action] — Owner: [Role], By: [Date]
3. [Action] — Owner: [Role], By: [Date]
```

### Section 8: Appendices

```markdown
# Appendices

## Appendix A: Complete Stakeholder List

| Name | Role | Department | Sessions | Key Contributions |
|------|------|------------|----------|-------------------|
| [Name] | [Title] | [Dept] | [X] | [Note] |

## Appendix B: Session Log

| Date | Session Type | Attendees | Topics Covered |
|------|--------------|-----------|----------------|
| [Date] | Discovery | [Names] | [Topics] |

## Appendix C: Systems Inventory Detail

| System | Version | Owner | Integrations | API Available |
|--------|---------|-------|--------------|---------------|
| [System] | [Ver] | [Owner] | [Systems] | Yes/No |

## Appendix D: Full Findings Data
[Detailed tables or data as needed]

## Appendix E: Glossary
| Term | Definition |
|------|------------|
| [Term] | [Definition] |
```

---

## Content Guidelines

### Writing Style

**Voice:**
- Professional and objective
- Evidence-based throughout
- Clear and direct
- Appropriate for technical readers

**Structure:**
- Lead with key point in each section
- Support with evidence
- End with implication/action
- Use headings liberally for scannability

**Tone:**
- Confident but measured
- Acknowledge uncertainty appropriately
- Avoid hyperbole
- Let evidence speak

### Level of Detail

| Section | Detail Level |
|---------|--------------|
| Executive Summary | High-level only |
| Findings | Detailed with evidence |
| Patterns | Analytical, synthesis |
| Recommendations | Specific and actionable |
| Appendices | Comprehensive reference |

### Handling Sensitive Information

- Confirm with client what can be documented
- Use role titles instead of names if needed
- Generalize specific data if confidential
- Mark sensitive sections if required

---

## Evidence Standards

### Citing Sources

Every finding must include evidence. Cite using this format:

**Direct quotes:**
> "The exact words spoken" — [Name, Role], [Session Date]

**Paraphrased observations:**
> Multiple stakeholders indicated [finding]. (Sessions: [Date], [Date])

**Document references:**
> Per [Document Name], [specific data or observation].

### Confidence Calibration

| Confidence | Evidence Standard | Presentation |
|------------|-------------------|--------------|
| High | 3+ sources, direct evidence | State as fact |
| Medium | 2 sources, reasonable inference | "Evidence suggests..." |
| Low | Single source, interpretation | "Initial signals indicate..." |

### What Requires Evidence

| Claim Type | Evidence Required |
|------------|-------------------|
| Factual statements | Direct quote or observation |
| Quantified claims | Data source or stakeholder estimate |
| Pattern claims | Multiple examples cited |
| Recommendations | Linked to specific findings |

---

## Formatting Standards

### Document Layout

**Page setup:**
- Margins: 1 inch all sides
- Page numbers: Bottom center
- Headers: Document title + section

**Typography:**
- Body: 11pt Inter/Helvetica/Arial
- Headings: 14pt/16pt/18pt bold
- Tables: 10pt

**Spacing:**
- 1.15 line spacing
- 12pt after paragraphs
- Page breaks before major sections

### Cadre Brand Application

**Cover page:**
- Client name: Deep Blue (#034377)
- Report title: Black
- Cadre logo: Bottom right

**Section headers:**
- Level 1: Deep Blue, 18pt bold
- Level 2: Black, 14pt bold
- Level 3: Black, 12pt bold

**Accent elements:**
- Tables: Deep Blue headers
- Key callouts: Coral Red (#DB4545) left border

**Tables:**
- Header row: Deep Blue background, white text
- Alternating rows: Light gray (#F5F5F5) / white
- Border: 1pt gray

---

## Generation Process

### Step 1: Gather All Data

```
1. Query data access → Get:
   - Client details (name, industry, size)
   - All stakeholders with archetypes
   - All sessions with dates
   - All findings across five dimensions
   - All challenges with scores
   - All solutions with scores

2. Run synthesis → Get:
   - Pattern analysis (dominant + emerging)
   - Gap analysis with coverage scores
   - Prioritized challenges
   - Prioritized solutions
   - Key insights
```

### Step 2: Structure Content

```
1. Group findings by dimension
2. Order findings by importance within each
3. Select patterns for analysis section
4. Rank recommendations
5. Build roadmap summary from prioritizer
```

### Step 3: Draft Report

```
1. Write executive summary last (after all sections)
2. Draft each section following templates
3. Include evidence for every finding
4. Ensure recommendations link to findings
5. Add appendices with reference data
```

### Step 4: Generate Document

```
1. Read docx/SKILL.md for creation workflow
2. Create document with proper styling
3. Apply Cadre brand elements
4. Generate table of contents
5. Add page numbers and headers
```

### Step 5: Quality Check

Before delivery, verify:
- [ ] Executive summary standalone readable
- [ ] Every finding has evidence cited
- [ ] Confidence levels calibrated correctly
- [ ] Patterns supported by evidence count
- [ ] Recommendations link to specific findings
- [ ] Roadmap aligns with recommendations
- [ ] Formatting consistent throughout
- [ ] No typos or errors
- [ ] Client name/details correct
- [ ] Table of contents accurate

---

## Complete Example

See appendix for full sample report. Key sections illustrated:

### Example: Finding with Evidence

```markdown
#### Finding PR3: Manual data transfer consumes 20+ hours weekly

**Confidence:** High

The organization's four core systems—ERP, CRM, field operations, and analytics—
operate in functional silos. Each handoff point between systems requires manual
data transfer, typically via spreadsheet export/import or manual re-entry.

This pattern affects multiple departments. Operations staff spend significant
time reconciling field data with the ERP system. Finance must manually pull
reports from multiple sources to compile weekly summaries. The analytics team
cannot access real-time data, limiting their ability to provide timely insights.

**Evidence:**
- "We have people whose entire job is copying data between spreadsheets. It's 
  not a good use of their skills." — Sarah Chen, VP Operations, Nov 15
- "I spend Monday mornings pulling the same five reports from three different 
  systems. It takes until noon." — David Park, Finance Analyst, Nov 18
- Operations team estimated 20+ hours weekly on data reconciliation tasks
  (Session: Nov 15)

**Implication:** Data integration is a foundational requirement before any AI
initiative can deliver value. Without unified data access, automation efforts
will be limited to single-system improvements rather than cross-functional
optimization.
```

### Example: Pattern Analysis

```markdown
### Pattern 1: Data Silos as Root Cause

**Strength:** Dominant (8 evidence points across 5 sessions)
**Dimensions:** Technology, Process, Challenges

Every major challenge identified traces back to disconnected systems. The lack
of integration forces manual workarounds that consume time, introduce errors,
and prevent real-time visibility. This pattern emerged consistently across
Operations, Finance, and Executive stakeholders.

**Evidence:**
| Source | Type | Finding |
|--------|------|---------|
| Sarah Chen, Nov 15 | Quote | "Four systems that don't talk to each other" |
| Marcus Webb, Nov 16 | Quote | "Integration was promised but never delivered" |
| David Park, Nov 18 | Observation | Manual report compilation process |
| Exec Session, Nov 20 | Quote | "We make decisions on 2-day-old data" |
| IT Review, Nov 16 | Document | No active integration between ERP/CRM |

**Implication:** Integration must be addressed early in the roadmap. Without
solving data silos, AI initiatives will have limited impact. However, the
failed 2023 integration project means this work must be approached carefully
to rebuild confidence.
```
